L2(x)
######################################################## Question 8 : Probabilité à postériori
prob_post1_1 = exp(L1(x))/(exp(L1(x)) + exp(L2(x)))
prob_post2_2 = exp(L2(x))/(exp(L1(x)) + exp(L2(x)))
prob_post1_1
prob_post2_2
#################################### Question 9 : Utilisation des fonctions lda et predict.lda
library(MASS)
lda = lda(y ~., data = train)
lda = lda(x = train[,2:3], grouping = as.factor(train$y))
lda$prior
lda$scaling # ????
predlda = predict(lda, x)
predlda$class
predlda$posterior
################################################### Question 10 : Representation graphique lda
predgrl_lda = predict(lda, grille)
plot(grille, pch = 20, col = predgrl_lda$class, cex = 0.5)
points(Xtrain, pch = Ytrain, col = Ytrain)
legend("topright", legend = c("Classe 1", "Classe 2"), col = 1:2, pch = 1:2)
########################################################## Question 11: Comparaison qda et lda
test = read.table("synth_test.txt", header = T)
Xtest = test[, 2:3]
Ytest = test$y
predy_qda = predict(qda, Xtest)
predy_lda = predict(lda, Xtest)
table(predy_qda$class, Ytest)
table(predy_lda$class, Ytest)
erreur_qda = sum(predy_qda$class != Ytest)/length(Ytest)
erreur_lda = sum(predy_lda$class != Ytest)/length(Ytest)
erreur_lda
erreur_qda
################################################################################### Exercice 2
############################################ Question 1 : Importation de données
Numtrain = read.table("numbers_train.txt", header = T)
Xntrain = as.matrix(Numtrain[,-1])
Yntrain = Numtrain$y
############################################## Question 2 : Visualisation d'images
par(mfrow=c(3,3))
for (i in 1:9){
image(matrix(Xntrain[i,],16,16), col=gray(1:100/100), ylim=c(1,0))
}
par(mfrow=c(1,1))
############################################# Question 3 : Prédiction avec lda
im_lda = lda(Numtrain[,-1], grouping = as.factor(Yntrain))
pred_im_lda = predict(im_lda,Numtrain[,-1])
table(pred_im_lda$class, Yntrain)
erreur_app = sum(pred_im_lda$class != Yntrain) * (1/length(Yntrain))
erreur_app
############################################## Question 4 : Importation des données tests :
Num_test = read.table("numbers_test.txt", header = T)
Xn_test = Num_test[,-1]
Yn_test = Num_test$y
########################################### Question 5 : Prédiction lda et calcul de l'erreur
pred_im_test_lda = predict(im_lda, Xn_test)
table(pred_im_test_lda$class, Yn_test)
erreur_im_test = sum(pred_im_test_lda$class != Yn_test)*(1/length(Yn_test))
########################################### Question 6 : Prédiction qda et calcul de l'erreur
im_qda = qda(Numtrain[,-1], grouping = as.factor(Yntrain))
################################################################################ Question 10 :
## Validation croisée LOO (Leave One Out)
# Charger la bibliothèque pour la validation croisée
library(MASS)
# Nombre d'observations
n <- nrow(Xtrain)
# Initialiser le compteur d'erreurs
error_count_loo <- 0
# Validation croisée Leave-One-Out (LOO)
for (i in 1:n) {
# Utiliser toutes les données sauf la i-ème pour l'entraînement
lda_model_loo <- lda(Xtrain[-i,], Ytrain[-i])
# Prédire la classe de la i-ème observation
prediction_loo <- predict(lda_model_loo, Xtrain[i, , drop = FALSE])$class
# Comparer la prédiction avec la vraie classe
if (prediction_loo != Ytrain[i]) {
error_count_loo <- error_count_loo + 1
}
}
# Calcul du taux d'erreur LOO
error_rate_loo <- error_count_loo / n
# Afficher le résultat
print(paste("Taux d'erreur LOO :", round(error_rate_loo * 100, 2), "%"))
#### validation croisée en 5 folds
# Nombre de folds
k <- 5
# Créer les indices pour les 5 folds
set.seed(123)  # Pour reproduire les résultats
folds <- sample(rep(1:k, length.out = length(Ytrain)))
# Initialiser le compteur d'erreurs
error_count_kfold <- 0
# Validation croisée 5-folds
for (i in 1:k) {
# Séparer les données en ensemble d'apprentissage et de test
train_idx <- which(folds != i)
test_idx <- which(folds == i)
# Ajuster le modèle LDA sur les données d'apprentissage
lda_model_kfold <- lda(Xtrain[train_idx, ], Ytrain[train_idx])
# Prédire les classes pour l'ensemble de test
predictions_kfold <- predict(lda_model_kfold, Xtrain[test_idx, ])$class
# Compter les erreurs de prédiction
error_count_kfold <- error_count_kfold + sum(predictions_kfold != Ytrain[test_idx])
}
# Calcul du taux d'erreur 5-folds
error_rate_kfold <- error_count_kfold / n
# Afficher le résultat
print(paste("Taux d'erreur 5-folds :", round(error_rate_kfold * 100, 2), "%"))
################################################################################### Exercice 3
## Implémentation de la fonction adq_estim
adq_estim <- function(X, Y) {
# Initialiser la liste des paramètres
K <- length(unique(Y))  # Nombre de classes
params <- vector("list", K)  # Liste pour stocker les paramètres pour chaque classe
# Calculer les paramètres pour chaque classe
for (k in 1:K) {
# Sélectionner les observations appartenant à la classe k
Xk <- X[Y == k, ]
nk <- nrow(Xk)  # Nombre d'observations dans la classe k
n <- nrow(X)  # Nombre total d'observations
# Estimer pi_k (probabilité a priori)
pi_k <- nk / n
# Estimer mu_k (moyenne)
mu_k <- colMeans(Xk)
# Estimer Sigma_k (matrice de covariance)
Sigma_k <- cov(Xk)
# Stocker les paramètres dans la liste
params[[k]] <- list(pi = pi_k, mu = mu_k, Sigma = Sigma_k)
}
# Retourner la liste des paramètres
return(params)
}
## Implémentation de la fonction adq_pred
adq_pred <- function(params, X_new) {
K <- length(params)  # Nombre de classes
n_new <- nrow(X_new)  # Nombre de nouvelles observations
posterior_probs <- matrix(0, n_new, K)  # Matrice pour stocker les probabilités à posteriori
# Calculer Q_k(x) pour chaque observation x et chaque classe k
for (k in 1:K) {
pi_k <- params[[k]]$pi
mu_k <- params[[k]]$mu
Sigma_k <- params[[k]]$Sigma
inv_Sigma_k <- solve(Sigma_k)  # Inverse de la matrice de covariance
det_Sigma_k <- det(Sigma_k)  # Déterminant de la matrice de covariance
for (i in 1:n_new) {
x_new <- X_new[i, ]
Q_k <- -0.5 * log(det_Sigma_k) - 0.5 * t(x_new - mu_k) %*% inv_Sigma_k %*% (x_new - mu_k) + log(pi_k)
posterior_probs[i, k] <- Q_k
}
}
# Calculer les probabilités à posteriori en exponentiant Q_k(x) et en normalisant
posterior_probs <- exp(posterior_probs)
posterior_probs <- posterior_probs / rowSums(posterior_probs)  # Normalisation
# Prédire la classe ayant la plus grande probabilité à posteriori
class_predictions <- apply(posterior_probs, 1, which.max)
# Retourner les classes prédites et les probabilités à posteriori
return(list(predictions = class_predictions, posterior = posterior_probs))
}
# Vérification des résultats avec l’exercice 1
# Estimation des paramètres sur les données d'apprentissage
params_adq <- adq_estim(Xtrain, Ytrain)
# Prédiction des classes pour de nouvelles observations
result_adq <- adq_pred(params_adq, Xtest)
################################################################################## Exercice 1
train = read.table("synth_train.txt", header = T)
Xtrain = train[,-1]
Ytrain = train$y
par(mfrow = c(1,1))
########################## Question 1 : Estimer ces paramètres sur les données d’apprentissage.
plot(Xtrain, pch = Ytrain, col = Ytrain)
legend("topright", legend = c("Classe 1","Classe 2"), pch = 1:2, col = 1:2)
pi = as.matrix(prop.table(table(Ytrain)))
pi1 = pi[1]
pi2 = pi[2]
bd1 = as.matrix(subset(train, y == 1, select = c(2:3)))
bd2 = as.matrix(subset(train, y == 2, select = c(2:3)))
mu1 = apply(bd1, 2, mean)
mu2 = apply(bd2, 2, mean)
n1 = nrow(bd1)
n2 = nrow(bd2)
y1 = sweep(bd1,2, mu1, "-")
y2 = sweep(bd2,2, mu2, "-")
sig1 = (1/(n1-1)) *(t(y1)%*%(y1)) # var(bd1)
sig2 = (1/(n2-1)) *(t(y2)%*%(y2)) # var(bd2)
#mu = cbind(tapply(Xtrain$x1, Ytrain, mean), tapply(Xtrain$x2, Ytrain, mean))
#mu1 = mu[1,]
#mu2 = mu[2,]
############################################################ Question 1 ( Autres alternatives)
# Calcul des paramètres pour la classe 1
class1 <- Xtrain[Ytrain == 1, ]
class2 <- Xtrain[Ytrain == 2, ]
# Estimation des moyennes
mu1 <- colMeans(class1)
mu2 <- colMeans(class2)
# Estimation des matrices de covariance
Sigma1 <- cov(class1)
Sigma2 <- cov(class2)
# Estimation des probabilités a priori
pi1 <- nrow(class1) / nrow(Xtrain)
pi2 <- nrow(class2) / nrow(Xtrain)
################################################ Question 2 : Fonction discrimante quadratique
Q1 = function(x){(-1/2) * log(det(sig1)) - (1/2) * (t(x-mu1)%*%solve(sig1)%*%(x-mu1)) + log(pi1)}
Q2 = function(x){(-1/2) * log(det(sig2)) - (1/2) * (t(x-mu2)%*%solve(sig2)%*%(x-mu2)) + log(pi2)}
x = c(-1,1)
Q1(x)
Q2(x)
####################### Question 3 : Estimation les probabilités à posteriori pour x = (−1, 1)
prob_post1 = exp(Q1(x))/(exp(Q1(x)) + exp(Q2(x)))
prob_post2 = exp(Q2(x))/(exp(Q1(x)) + exp(Q2(x)))
prob_post1
prob_post2
#################################### Question 4 : Utilisation des fonctions qda et predict.qda
library(MASS)
qda = qda(y ~., data = train)
qda = qda(x = train[,2:3], grouping = as.factor(train$y) )
qda$prior
qda$scaling
predqda = predict(qda, x)
predqda$class
predqda$posterior
################################################ Question 5 : représentation de la méthode qda
load("grille.rda")
predgr = predict(qda, grille)
plot(grille, pch = 20, col = predgr$class, cex = 0.5)
points(Xtrain, pch = Ytrain, col = Ytrain)
legend("topright", legend = c("Classe 1", "Classe 2"), col = 1:2, pch = 1:2)
################################################ Question 6 : représentation de la méthode qda
N = nrow(train)
sig = (1/N) * (n1*sig1 + n2*sig2)
sig
## Question 7 : La règle de décision de Bayes (prédire la classe la plus probable à posteriori)
L1 = function(x){t(x)%*%solve(sig)%*%mu1 - (1/2)*(t(mu1)%*%solve(sig)%*%mu1) + log(pi1)}
L2 = function(x){t(x)%*%solve(sig)%*%mu2 - (1/2)*(t(mu2)%*%solve(sig)%*%mu2) + log(pi2)}
L1(x)
L2(x)
######################################################## Question 8 : Probabilité à postériori
prob_post1_1 = exp(L1(x))/(exp(L1(x)) + exp(L2(x)))
prob_post2_2 = exp(L2(x))/(exp(L1(x)) + exp(L2(x)))
prob_post1_1
prob_post2_2
#################################### Question 9 : Utilisation des fonctions lda et predict.lda
library(MASS)
lda = lda(y ~., data = train)
lda = lda(x = train[,2:3], grouping = as.factor(train$y))
lda$prior
lda$scaling # ????
predlda = predict(lda, x)
predlda$class
predlda$posterior
################################################### Question 10 : Representation graphique lda
predgrl_lda = predict(lda, grille)
plot(grille, pch = 20, col = predgrl_lda$class, cex = 0.5)
points(Xtrain, pch = Ytrain, col = Ytrain)
legend("topright", legend = c("Classe 1", "Classe 2"), col = 1:2, pch = 1:2)
########################################################## Question 11: Comparaison qda et lda
test = read.table("synth_test.txt", header = T)
Xtest = test[, 2:3]
Ytest = test$y
predy_qda = predict(qda, Xtest)
predy_lda = predict(lda, Xtest)
table(predy_qda$class, Ytest)
table(predy_lda$class, Ytest)
erreur_qda = sum(predy_qda$class != Ytest)/length(Ytest)
erreur_lda = sum(predy_lda$class != Ytest)/length(Ytest)
erreur_lda
erreur_qda
################################################################################### Exercice 2
############################################ Question 1 : Importation de données
Numtrain = read.table("numbers_train.txt", header = T)
Xntrain = as.matrix(Numtrain[,-1])
Yntrain = Numtrain$y
############################################## Question 2 : Visualisation d'images
par(mfrow=c(3,3))
for (i in 1:9){
image(matrix(Xntrain[i,],16,16), col=gray(1:100/100), ylim=c(1,0))
}
par(mfrow=c(1,1))
############################################# Question 3 : Prédiction avec lda
im_lda = lda(Numtrain[,-1], grouping = as.factor(Yntrain))
pred_im_lda = predict(im_lda,Numtrain[,-1])
table(pred_im_lda$class, Yntrain)
erreur_app = sum(pred_im_lda$class != Yntrain) * (1/length(Yntrain))
erreur_app
############################################## Question 4 : Importation des données tests :
Num_test = read.table("numbers_test.txt", header = T)
Xn_test = Num_test[,-1]
Yn_test = Num_test$y
########################################### Question 5 : Prédiction lda et calcul de l'erreur
pred_im_test_lda = predict(im_lda, Xn_test)
table(pred_im_test_lda$class, Yn_test)
erreur_im_test = sum(pred_im_test_lda$class != Yn_test)*(1/length(Yn_test))
########################################### Question 6 : Prédiction qda et calcul de l'erreur
# im_qda = qda(Numtrain[,-1], grouping = as.factor(Yntrain))
# pred_im_test_qda = predict(im_qda, Xn_test)
# table(pred_im_test_lda$class, Yn_test)
# erreur_im_test = sum(pred_im_test_lda$class != Yn_test)*(1/length(Yn_test))
# erreur_im_test
################################################################################ Question 10 :
## Validation croisée LOO (Leave One Out)
# Charger la bibliothèque pour la validation croisée
library(MASS)
# Nombre d'observations
n <- nrow(Xtrain)
# Initialiser le compteur d'erreurs
error_count_loo <- 0
# Validation croisée Leave-One-Out (LOO)
for (i in 1:n) {
# Utiliser toutes les données sauf la i-ème pour l'entraînement
lda_model_loo <- lda(Xtrain[-i,], Ytrain[-i])
# Prédire la classe de la i-ème observation
prediction_loo <- predict(lda_model_loo, Xtrain[i, , drop = FALSE])$class
# Comparer la prédiction avec la vraie classe
if (prediction_loo != Ytrain[i]) {
error_count_loo <- error_count_loo + 1
}
}
# Calcul du taux d'erreur LOO
error_rate_loo <- error_count_loo / n
# Afficher le résultat
print(paste("Taux d'erreur LOO :", round(error_rate_loo * 100, 2), "%"))
#### validation croisée en 5 folds
# Nombre de folds
k <- 5
# Créer les indices pour les 5 folds
set.seed(123)  # Pour reproduire les résultats
folds <- sample(rep(1:k, length.out = length(Ytrain)))
# Initialiser le compteur d'erreurs
error_count_kfold <- 0
# Validation croisée 5-folds
for (i in 1:k) {
# Séparer les données en ensemble d'apprentissage et de test
train_idx <- which(folds != i)
test_idx <- which(folds == i)
# Ajuster le modèle LDA sur les données d'apprentissage
lda_model_kfold <- lda(Xtrain[train_idx, ], Ytrain[train_idx])
# Prédire les classes pour l'ensemble de test
predictions_kfold <- predict(lda_model_kfold, Xtrain[test_idx, ])$class
# Compter les erreurs de prédiction
error_count_kfold <- error_count_kfold + sum(predictions_kfold != Ytrain[test_idx])
}
# Calcul du taux d'erreur 5-folds
error_rate_kfold <- error_count_kfold / n
# Afficher le résultat
print(paste("Taux d'erreur 5-folds :", round(error_rate_kfold * 100, 2), "%"))
################################################################################### Exercice 3
## Implémentation de la fonction adq_estim
adq_estim <- function(X, Y) {
# Initialiser la liste des paramètres
K <- length(unique(Y))  # Nombre de classes
params <- vector("list", K)  # Liste pour stocker les paramètres pour chaque classe
# Calculer les paramètres pour chaque classe
for (k in 1:K) {
# Sélectionner les observations appartenant à la classe k
Xk <- X[Y == k, ]
nk <- nrow(Xk)  # Nombre d'observations dans la classe k
n <- nrow(X)  # Nombre total d'observations
# Estimer pi_k (probabilité a priori)
pi_k <- nk / n
# Estimer mu_k (moyenne)
mu_k <- colMeans(Xk)
# Estimer Sigma_k (matrice de covariance)
Sigma_k <- cov(Xk)
# Stocker les paramètres dans la liste
params[[k]] <- list(pi = pi_k, mu = mu_k, Sigma = Sigma_k)
}
# Retourner la liste des paramètres
return(params)
}
## Implémentation de la fonction adq_pred
adq_pred <- function(params, X_new) {
K <- length(params)  # Nombre de classes
n_new <- nrow(X_new)  # Nombre de nouvelles observations
posterior_probs <- matrix(0, n_new, K)  # Matrice pour stocker les probabilités à posteriori
# Calculer Q_k(x) pour chaque observation x et chaque classe k
for (k in 1:K) {
pi_k <- params[[k]]$pi
mu_k <- params[[k]]$mu
Sigma_k <- params[[k]]$Sigma
inv_Sigma_k <- solve(Sigma_k)  # Inverse de la matrice de covariance
det_Sigma_k <- det(Sigma_k)  # Déterminant de la matrice de covariance
for (i in 1:n_new) {
x_new <- X_new[i, ]
Q_k <- -0.5 * log(det_Sigma_k) - 0.5 * t(x_new - mu_k) %*% inv_Sigma_k %*% (x_new - mu_k) + log(pi_k)
posterior_probs[i, k] <- Q_k
}
}
# Calculer les probabilités à posteriori en exponentiant Q_k(x) et en normalisant
posterior_probs <- exp(posterior_probs)
posterior_probs <- posterior_probs / rowSums(posterior_probs)  # Normalisation
# Prédire la classe ayant la plus grande probabilité à posteriori
class_predictions <- apply(posterior_probs, 1, which.max)
# Retourner les classes prédites et les probabilités à posteriori
return(list(predictions = class_predictions, posterior = posterior_probs))
}
# Vérification des résultats avec l’exercice 1
# Estimation des paramètres sur les données d'apprentissage
params_adq <- adq_estim(Xtrain, Ytrain)
# Prédiction des classes pour de nouvelles observations
result_adq <- adq_pred(params_adq, Xtest)
# Estimation des paramètres sur les données d'apprentissage
params_adq <- adq_estim(Xtrain, Ytrain)
# Prédiction des classes pour de nouvelles observations
result_adq <- adq_pred(params_adq, Xtest)
params_adq
params_adq$Sigma
result_adq <- adq_pred(params_adq, Xtest)
load("Desbois_complet.rda")
colnames(data)
## Question 1
library(MASS)
# Indices des 75% de données d'apprentissage
set.seed(10)
tr = sample(1:nrow(data),945)
tr
# Métode lda avec la syntaxe utilisant les formules
g = lda(DIFF~., data=data[tr,])
pred = predict(g, data[-tr,-1])$class
sum(pred != data[-tr,1])/length(pred)
# Taux d'erreur test (pour ce découpage) de 15.2%
################################################################################ Question 2
library(klaR)
?greedy.wilks
formula = greedy.wilks(DIFF~., data=data[tr,], niveau = 0.2)$formula
formula
g = lda(formula, data=data[tr,])
pred = predict(g, data[-tr,-1])$class
sum(pred != data[-tr,1])/length(pred)
# Taux d'erreur test (avec ces variables et pour ce découpage) de 14.6%
# Test 1 greedy wilks
base = c() # matrix vide
lda = c()
lda_wilks = c()
for (i in 1:100){
tr = sample(1:nrow(data),945)
g_lda = lda(DIFF~., data=data[tr,])
pred = predict(g_lda, data[-tr,-1])$class
err_lda = sum(pred != data[-tr,1])/length(pred)
wilks = greedy.wilks(DIFF~., data=data[tr,], niveau = 0.2)
g_lda_wilks = lda(wilks$formula, data=data[tr,])
pred_wilks = predict(g_lda_wilks, data[-tr,-1])$class
err_lda_wilks = sum(pred_wilks != data[-tr,1])/length(pred)
base = rbind(base, wilks$results[1])
lda = rbind(lda, err_lda)
lda_wilks = rbind(lda_wilks, err_lda_wilks)
}
ind = order(table(base), decreasing = T)
barplot(table(base)[ind])
###################################################################### Exercice 4 : Alternative
############################################## Question 1
# Charger les données
load("Desbois_complet.rda")
# Définir la variable cible et les variables explicatives
Y <- data$DIFF  # Variable qualitative (0 = sain, 1 = défaillant)
X <- data[, -1]  # Les 22 variables quantitatives
# Séparer les données en ensemble d'apprentissage et de test (75 % - 25 %)
set.seed(10)  # Pour rendre les résultats reproductibles
tr <- sample(1:nrow(data), 0.75 * nrow(data))
# Ajuster le modèle LDA sur les données d'apprentissage
library(MASS)
lda_model <- lda(DIFF ~ ., data = data[tr, ])
# Prédire les classes pour l'ensemble de test
pred <- predict(lda_model, data[-tr, -1])$class
# Calcul du taux d'erreur sur les données de test
error_rate <- sum(pred != data[-tr, 1]) / length(pred)
# Affichage du taux d'erreur
print(paste("Taux d'erreur sur l'ensemble de test :", round(error_rate * 100, 2), "%"))
########################################## Question 2
# Charger la bibliothèque klar pour utiliser greedy.wilks
library(klaR)
# Appliquer la méthode greedy.wilks pour sélectionner les variables
greedy_result <- greedy.wilks(DIFF ~ ., data = data[tr, ])
# Afficher la formule des variables sélectionnées
formula_selected <- greedy_result$formula
print(formula_selected)
# Ajuster le modèle LDA avec les variables sélectionnées
lda_model_selected <- lda(formula_selected, data = data[tr, ])
# Prédire les classes sur les données de test
pred_selected <- predict(lda_model_selected, data[-tr, -1])$class
# Calcul du taux d'erreur avec les variables sélectionnées
error_rate_selected <- sum(pred_selected != data[-tr, 1]) / length(pred_selected)
# Affichage du taux d'erreur
print(paste("Taux d'erreur avec les variables sélectionnées :", round(error_rate_selected * 100, 2), "%"))
################################################### Question 3
# Modifier le paramètre 'niveau' dans greedy.wilks
greedy_result_niveau <- greedy.wilks(DIFF ~ ., data = data[tr, ], niveau = 0.01)
# Ajuster le modèle avec les variables sélectionnées
lda_model_niveau <- lda(greedy_result_niveau$formula, data = data[tr, ])
# Prédire sur les données de test
pred_niveau <- predict(lda_model_niveau, data[-tr, -1])$class
# Calculer le taux d'erreur
error_rate_niveau <- sum(pred_niveau != data[-tr, 1]) / length(pred_niveau)
# Affichage du taux d'erreur
print(paste("Taux d'erreur avec 'niveau = 0.01' :", round(error_rate_niveau * 100, 2), "%"))
############################################# Question 4
# Nouvelle séparation apprentissage/test
set.seed(123)  # Nouvelle graine pour un nouveau découpage
tr_new <- sample(1:nrow(data), 0.75 * nrow(data))
# Réappliquer la méthode greedy.wilks
greedy_result_new <- greedy.wilks(DIFF ~ ., data = data[tr_new, ])
# Ajuster le modèle LDA avec les nouvelles variables sélectionnées
lda_model_new <- lda(greedy_result_new$formula, data = data[tr_new, ])
# Prédire les classes sur les nouvelles données de test
pred_new <- predict(lda_model_new, data[-tr_new, -1])$class
# Calculer le taux d'erreur avec le nouveau découpage
error_rate_new <- sum(pred_new != data[-tr_new, 1]) / length(pred_new)
# Affichage du taux d'erreur avec le nouveau découpage
print(paste("Taux d'erreur avec le nouveau découpage :", round(error_rate_new * 100, 2), "%"))
############################################ Question 5
# Prédire la classe de la 3ème exploitation agricole
new_obs <- data[3, -1, drop = FALSE]  # Extraire les variables explicatives de la 3ème exploitation
prediction_new_obs <- predict(lda_model_new, new_obs)$class
# Affichage de la prédiction
print(paste("La 3ème exploitation est prédite comme :", ifelse(prediction_new_obs == 1, "Défaillante", "Saine")))
barplot(table(base)[ind])
